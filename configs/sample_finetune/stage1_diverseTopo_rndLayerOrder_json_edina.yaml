data:
  train_bs: 1
  validation_bs: 1
  train_width: 256
  train_height: 256
  train_val_max_img: 2
  validation_val_max_img: 4
#   meta_paths:
#     - "./data/fashion_meta.json"
#   # Margin of frame indexes between ref and tgt images
#   sample_margin: 30  

# right feet 1
# right knee 2
# neck 3
# left knee 4
# left feet 5
# right wrist 6
# head 7
 
train_data:
  target: src.dataset.ChDataset_GaussianSkeleton_DiverseAppearance_json_rndLayerOrder.ChDataset
  params:
    json_path: ./dataset/edina/train.json
    # need to be in order from back to forth.
    target_joints: ['6','7','10','11','1','14','15','2','12','13','3','4','5','8','9']
    seed: 42
    dataset_size: 3200
    mod: 3
    #########fixed for json datasets
    sample_n_frames: 0
    sample_size:     256
    sample_stride:   1
    n_pairs: 0
    composite_by_add: False
    everywhere_cartoon_prob: 0
    color_aug_prob: 0
    change_composite_order_prob: 0
    #########
    #probability of some augmentations
    flip_prob: 0.
    resize_prob: 0.
    skeleton_layer_order: True
    rotate_translate_prob: 0.5
    # background_dir: ['white','remove_anything']

    draw_joint_skeleton: "displacement"
    bone_std: 4 #half
    joint_std: 4 #radius
    target_bg_color: "white"
    skeleton_bg_color: "black"
    #just to make things easier: if certain paths need to be replace for all data,
    #put it here: this list will be used as path_str.replace(list[0], list[1])
    replace_path: ["/Users/zeqi_school/Downloads/", "./dataset/"]

validation_data:
  # target: animatediff.data.DiverseTopo.ChDataset
  target: src.dataset.ChDataset_GaussianSkeleton_DiverseAppearance_json_rndLayerOrder.ChDataset
  params:
    json_path: ./dataset/edina/test.json
    target_joints: ['6','7','10','11','1','14','15','2','12','13','3','4','5','8','9']
    # ['1','14','15','2','12','13','3','4','5','6','7','8','9','10','11']
    seed: 42
    # dataset_size: 200
    # mod: 6
    sample_n_frames: 0
    sample_size:     256
    sample_stride:   1
    n_pairs: 0
    flip_prob: 0.
    resize_prob: 0.
    composite_by_add: False
    everywhere_cartoon_prob: 0
    color_aug_prob: 0
    change_composite_order_prob: 0
    skeleton_layer_order: True
    rotate_translate_prob: 0.5
    # background_dir: ['white','remove_anything']

    draw_joint_skeleton: "displacement"
    bone_std: 4 #half
    joint_std: 4 #radius
    target_bg_color: "white"
    skeleton_bg_color: "black"
    replace_path: ["/Users/zeqi_school/Downloads/", "./dataset/"]


solver:
  gradient_accumulation_steps: 1
  mixed_precision: 'fp16'
  enable_xformers_memory_efficient_attention: True 
  gradient_checkpointing: False 
  max_train_steps: 32000 #should be 32000 for the default version
  max_grad_norm: 1.0
  # lr
  learning_rate: 1.0e-5
  scale_lr: False 
  lr_warmup_steps: 1
  lr_scheduler: 'constant'

  # optimizer
  use_8bit_adam: True
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_weight_decay:  1.0e-2
  adam_epsilon: 1.0e-8

val:
  validation_steps: 500


noise_scheduler_kwargs:
  num_train_timesteps: 1000
  beta_start:          0.00085
  beta_end:            0.012
  beta_schedule:       "scaled_linear"
  steps_offset:        1
  clip_sample:         false

base_model_path: './pretrained_weights/sd-image-variations-diffusers'
vae_model_path: './pretrained_weights/sd-vae-ft-mse'
image_encoder_path: './pretrained_weights/sd-image-variations-diffusers/image_encoder'
controlnet_openpose_path: './pretrained_weights/control_v11p_sd15_openpose/diffusion_pytorch_model.bin'
denoising_unet_path: ""
reference_unet_path: ""
pose_guider_path: ""


weight_dtype: 'fp16'  # [fp16, fp32]
uncond_ratio: 0.1
noise_offset: 0.05
snr_gamma: 5.0
enable_zero_snr: True 
pose_guider_pretrain: False #True 
# train_batch_size: 2

seed: 12580
resume_from_checkpoint: 'latest'
checkpointing_steps: 2000
save_model_epoch_interval: 5
save_name: 'finetune_edina'
exp_name: 'stage1'
output_dir: './results/rndLayerOrder_512'